{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# ì‹œê°í™” ê²°ê³¼ê°€ ì„ ëª…í•˜ê²Œ í‘œì‹œë˜ë„ë¡\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "df = pd.read_csv(\"word2vec_wrangling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exercise_name</th>\n",
       "      <th>Content_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PT</td>\n",
       "      <td>ğŸ’¯ What I try to educate my clients around, doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ê²€ë„</td>\n",
       "      <td>#20200115\\nì €ë… ì´ˆëŒ€!\\nì™€ì¸ì” ì†ì— ë¹„ì¹˜ëŠ”\\nëª¨ë“  ê²ƒë“¤ì´ í™”ë ¤í•œ\\në„ì‹¬ì†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ê¸°êµ¬í•„ë¼í…ŒìŠ¤</td>\n",
       "      <td>#ì˜¤ëŠ˜ì˜ë™ì‘\\nìºë”œë½ ë™ì‘ì˜ ì™„ì„± 'í–‰ì‰'\\nâ €\\nì¤‘ë ¥ì„ ì´ìš©í•´ ì²™ì¶”ë¥¼ ëŠ˜ë ¤ì£¼ê³ \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ë‹¤ë¹ˆì¹˜ë°”ë””ë³´ë“œ</td>\n",
       "      <td>#mbnìƒìƒì •ë³´ë§ˆë‹¹ \\n#ê³ íˆ¬\\n#ê³ íˆ¬GX\\n#ë‹¤ë¹ˆì¹˜ë°”ë””ë³´ë“œ\\n#ìƒë°©ì†¡ #GOTOL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ë“œëŸ¼ìŠ¤í‹±</td>\n",
       "      <td>#ë“œëŸ¼ìŠ¤í‹± #ê³ ë¬´íŒ #í…Œí¬ë¼ìŠ¤í‹± #ì „ìë“œëŸ¼ìš©ìŠ¤í‹±\\n\\n1. ì „ìë“œëŸ¼íƒ€ê²©ì‹œ ëœ ì‹œë„ëŸ½...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exercise_name                                        Content_txt\n",
       "0            PT  ğŸ’¯ What I try to educate my clients around, doe...\n",
       "1            ê²€ë„  #20200115\\nì €ë… ì´ˆëŒ€!\\nì™€ì¸ì” ì†ì— ë¹„ì¹˜ëŠ”\\nëª¨ë“  ê²ƒë“¤ì´ í™”ë ¤í•œ\\në„ì‹¬ì†...\n",
       "2        ê¸°êµ¬í•„ë¼í…ŒìŠ¤  #ì˜¤ëŠ˜ì˜ë™ì‘\\nìºë”œë½ ë™ì‘ì˜ ì™„ì„± 'í–‰ì‰'\\nâ €\\nì¤‘ë ¥ì„ ì´ìš©í•´ ì²™ì¶”ë¥¼ ëŠ˜ë ¤ì£¼ê³ \\n...\n",
       "3       ë‹¤ë¹ˆì¹˜ë°”ë””ë³´ë“œ  #mbnìƒìƒì •ë³´ë§ˆë‹¹ \\n#ê³ íˆ¬\\n#ê³ íˆ¬GX\\n#ë‹¤ë¹ˆì¹˜ë°”ë””ë³´ë“œ\\n#ìƒë°©ì†¡ #GOTOL...\n",
       "4          ë“œëŸ¼ìŠ¤í‹±  #ë“œëŸ¼ìŠ¤í‹± #ê³ ë¬´íŒ #í…Œí¬ë¼ìŠ¤í‹± #ì „ìë“œëŸ¼ìš©ìŠ¤í‹±\\n\\n1. ì „ìë“œëŸ¼íƒ€ê²©ì‹œ ëœ ì‹œë„ëŸ½..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soynlp\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # ê°œí–‰ë¬¸ì ì œê±°\n",
    "    text = re.sub('\\\\\\\\n', ' ', text)\n",
    "    # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    # íŠ¹ìˆ˜ë¬¸ìë‚˜ ì´ëª¨í‹°ì½˜ ë“±ì€ ë•Œë¡œëŠ” ì˜ë¯¸ë¥¼ ê°–ê¸°ë„ í•˜ì§€ë§Œ ì—¬ê¸°ì—ì„œëŠ” ì œê±°í–ˆìŠµë‹ˆë‹¤.\n",
    "    # text = re.sub('[?.,;:|\\)*~`â€™!^\\-_+<>@\\#$%&-=#}â€»]', '', text)\n",
    "    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "    # text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9]', ' ', text)\n",
    "    # í•œê¸€, ì˜ë¬¸ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "    text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(exercise_name):\n",
    "    file_name = \"#\" + exercise_name + \"_sum.txt\"\n",
    "    file_1 = \"/Users/noopy/FitCuration/\" + file_name\n",
    "    text = open(file_1, 'r', -1, \"UTF-8\", errors=\"ignore\").read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = open_file(\"ìì´ë¡œí† ë‹‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   ìì´ë¡œí† ë‹‰  gyrotonic ë¹„êµí•  ìˆ˜ ì—†ëŠ” ë ˆìŠ¨ í€„ë¦¬í‹°     ì›” í”„ë¡¬ê³¼ í•¨ê»˜í•˜ì„¸ìš”   ìœ„ë¡€ì‹ ë„ì‹œ No      ì›” ì´ë²¤íŠ¸  ì‹ ê·œ ê°œì¸ë ˆìŠ¨   íšŒ   ë§Œì› ì„ ì°©ìˆœ   ë¶„   ì‹ ê·œ ê·¸ë£¹ë ˆìŠ¨ ìµœëŒ€    í• ì¸ ê°œì›”ìˆ˜ ë³„ë¡œ í• ì¸ìœ¨ ìƒì´    ì¸ ì´ìƒ ë‹¨ì²´ë“±ë¡ ì‹œ ì‹ ê·œí´ë˜ìŠ¤ ì˜¤í”ˆ      ì¸ ì†Œìˆ˜ ê·¸ë£¹ë ˆìŠ¨ ë¯¸êµ­ BBU ì¥ë¹„        ê°œì¸ë ˆìŠ¨ ë¯¸êµ­ Gratz  BBU  Gyrotonic ì¥ë¹„     í´ë˜ì‹í•„ë¼í…ŒìŠ¤  ìì´ë¡œí† ë‹‰    í”„ë¡¬í•„ë¼í…ŒìŠ¤ ìœ„ë¡€ë™ë¡œ     ìš°ì„±ë©”ë””í”¼ì•„  ì¸µ                  ì•ˆë…•í•˜ì„¸ìš”  ì—ì½”í•„ë¼í…ŒìŠ¤ í”ŒëŸ¬ìŠ¤ì…ë‹ˆë‹¤     ì„¤ ì—°íœ´ ì „ê¹Œì§€ ì—´ì‹¬íˆ ìš´ë™í•˜ëŸ¬ ì˜¤ì‹œëŠ” ìš°ë¦¬ ë©‹ì° íšŒì›ë‹˜ë“¤ì„ ë§ì´í•˜ë ¤ ì €ëŠ” ì¶œê·¼í•˜ìë§ˆì ë¶€ë´ë¶€ë´ ì²­ì†Œë¥¼ ëë§ˆì³¤ë‹µë‹ˆë‹¤      ì–´ì„œ ì„¼í„°ë¥¼ ëœ¨ê²ê²Œ ë‹¬êµ¬ì–´ ì£¼ì„¸ìš”     ì˜¤ëŠ˜ë„ í‰í™”ë¡œìš´ ì—ì½”í•„ë¼í…ŒìŠ¤ í”ŒëŸ¬ìŠ¤ì™€ í•¨ê»˜        ìì„¸í•œ ë¬¸ì˜ ë° ìƒë‹´                 ì¹´ì¹´ì˜¤ ì˜¤í”ˆì±„íŒ…   ì—ì½”í•„ë¼í…ŒìŠ¤í”ŒëŸ¬ìŠ¤   blog naver com ecopilatesplus   ì²¨ë¨¹ì–´ë³´ëŠ” ì¶°ì»¬ë¦¿    ì‚¬ë§Œë‹¤ ì—ë“€ì¼€ì´í„°  samantha jrobinsonhk ì˜ ë‹¬ë‹¬í•œê°„ì‹   ë§›ìˆë‹¤   ã…ã…ìê¾¸ë¨¹ê²Œë˜ëŠ”êµ°ìš”      ì•„ì¹˜ì•¤ì»¬  í•„ë¼í…ŒìŠ¤  í´ìŠ¤íƒ€í•„ë¼í…ŒìŠ¤  ìì´ë¡œí† ë‹‰  ìì´ë¡œí† ë‹‰   ìì´ë¡œí‚¤ë„¤ì‹œìŠ¤  ìì´ë¡œí‚¤ë„¤ì‹œìŠ¤   ì²­ë‹´í•„ë¼í…ŒìŠ¤  ì²­ë‹´ìì´ë¡œí† ë‹‰  ë¶€ì‚°í•„ë¼í…ŒìŠ¤  ë¶€ì‚°ìì´ë¡œí† ë‹‰  archncurl  pilates  polestarpilates  gyrotonic  gyrokinesis íš¨ì§€ì›ì¥ë‹˜  misa limepilates ì˜ ì„ ë¬¼    êµìœ¡ë“£ëŠë¼ ì•ˆê·¸ë˜ë„ ì •ì‹ ì—†ì„í…ë° ìš”ë ‡ê²Œ ë˜ ì±™ê²¨ì£¼ë‹ˆ ë„˜ ê³ ë§ˆì›Œìš” ì´ì˜ê²Œ ì˜ì“¸ê²Œìš©         ì•„ì¹˜ì•¤ì»¬  í•„ë¼í…ŒìŠ¤  í´ìŠ¤íƒ€í•„ë¼í…ŒìŠ¤  ìì´ë¡œí† ë‹‰  ìì´ë¡œí† ë‹‰ê°•ì‚¬  ìì´ë¡œí‚¤ë„¤ì‹œìŠ¤  ìì´ë¡œí‚¤ë„¤ì‹œìŠ¤   ì²­ë‹´í•„ë¼í…ŒìŠ¤  ë¶€ì‚°í•„ë¼í…ŒìŠ¤  ì²­ë‹´ìì´ë¡œí† ë‹‰  ë¶€ì‚°ìì´ë¡œí† ë‹‰  archncurl  pilates  polestarpilates  gyrotonic  gyrokinesis  ì§€ìœ í•„ë¼í…ŒìŠ¤ ìì´ë¡œí† ë‹‰ G YOU '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_content = preprocessing(sample_content)\n",
    "sample_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec ëª¨ë¸ í•™ìŠµì— ë¡œê·¸ë¥¼ ì°ì„ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 948 ms, sys: 13.1 ms, total: 961 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "# %timeì„ ì°ì–´ì£¼ë©´ í•´ë‹¹ ì½”ë“œë¥¼ ì‹¤í–‰í•  ë•Œ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¶œë ¥í•´ ì¤ë‹ˆë‹¤\n",
    "%time sentences = df['Content_txt'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noopy/opt/anaconda3/lib/python3.7/site-packages/soynlp/tokenizer/_tokenizer.py:19: FutureWarning: Possible nested set at position 13\n",
      "  ('english & latin', re.compile(u\"[a-zA-ZÃ€-Ã¿]+[[`']?s]*|[a-zA-ZÃ€-Ã¿]+\", re.UNICODE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<soynlp.tokenizer._tokenizer.RegexTokenizer at 0x7fecc0b55210>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìì´ë¡œí† ë‹‰', 'gyrotonic', 'ë¹„êµí• ', 'ìˆ˜', 'ì—†ëŠ”', 'ë ˆìŠ¨', 'í€„ë¦¬í‹°', 'ì›”', 'í”„ë¡¬ê³¼', 'í•¨ê»˜í•˜ì„¸ìš”']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì „ì²˜ë¦¬ ì´í›„ì˜ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ í† í°í™”\n",
    "tokened_content = tokenizer.tokenize(sample_content)\n",
    "tokened_content[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43663\n"
     ]
    }
   ],
   "source": [
    "print(len(tokened_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 00:32:23,593 : INFO : collecting all words and their counts\n",
      "2020-02-03 00:32:23,594 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 23.9 ms, total: 16.6 s\n",
      "Wall time: 16.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 00:32:24,153 : INFO : collected 324341 word types from a corpus of 2300597 raw words and 61 sentences\n",
      "2020-02-03 00:32:24,154 : INFO : Loading a fresh vocabulary\n",
      "2020-02-03 00:32:25,685 : INFO : effective_min_count=1 retains 324341 unique words (100% of original 324341, drops 0)\n",
      "2020-02-03 00:32:25,686 : INFO : effective_min_count=1 leaves 2300597 word corpus (100% of original 2300597, drops 0)\n",
      "2020-02-03 00:32:26,489 : INFO : deleting the raw counts dictionary of 324341 items\n",
      "2020-02-03 00:32:26,494 : INFO : sample=0.001 downsamples 9 most-common words\n",
      "2020-02-03 00:32:26,495 : INFO : downsampling leaves estimated 2287695 word corpus (99.4% of prior 2300597)\n",
      "2020-02-03 00:32:27,172 : INFO : estimated required memory for 324341 words and 100 dimensions: 421643300 bytes\n",
      "2020-02-03 00:32:27,173 : INFO : resetting layer weights\n",
      "2020-02-03 00:33:14,694 : INFO : training model with 3 workers on 324341 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-02-03 00:33:15,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-03 00:33:15,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-03 00:33:15,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-03 00:33:15,241 : INFO : EPOCH - 1 : training on 2300597 raw words (569252 effective words) took 0.5s, 1043847 effective words/s\n",
      "2020-02-03 00:33:15,771 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-03 00:33:15,787 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-03 00:33:15,794 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-03 00:33:15,794 : INFO : EPOCH - 2 : training on 2300597 raw words (569235 effective words) took 0.5s, 1036958 effective words/s\n",
      "2020-02-03 00:33:16,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-03 00:33:16,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-03 00:33:16,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-03 00:33:16,375 : INFO : EPOCH - 3 : training on 2300597 raw words (569251 effective words) took 0.6s, 988706 effective words/s\n",
      "2020-02-03 00:33:16,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-03 00:33:16,945 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-03 00:33:16,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-03 00:33:16,952 : INFO : EPOCH - 4 : training on 2300597 raw words (569251 effective words) took 0.6s, 994865 effective words/s\n",
      "2020-02-03 00:33:17,514 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-02-03 00:33:17,529 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-02-03 00:33:17,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-02-03 00:33:17,536 : INFO : EPOCH - 5 : training on 2300597 raw words (569239 effective words) took 0.6s, 982860 effective words/s\n",
      "2020-02-03 00:33:17,537 : INFO : training on a 11502985 raw words (2846228 effective words) took 2.8s, 1001453 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fb3fe01b750>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´ˆê¸°í™” ë° ëª¨ë¸ í•™ìŠµ\n",
    "from gensim.models import word2vec\n",
    "\n",
    "%time tokens = sentences.apply(tokenizer.tokenize)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = word2vec.Word2Vec(tokens, min_count=1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_Twitter = [\"ì…ë‹ˆë‹¤\",\"ìˆëŠ”\",\"ìˆìŠµë‹ˆë‹¤\",\"ê°™ì€\",\"ì•ˆë…•í•˜ì„¸ìš”\",\"ê³ ë§ˆì›Œìš”\",\"ìˆì–´ìš”\",\"ìˆê²Œ\"\\\n",
    "                     ,\"ìˆë„ë¡\",\"ë¶€íƒë“œë¦½ë‹ˆë‹¤\",\"í•˜ëŠ”\",\"í•©ë‹ˆë‹¤\",\"í• \",\"í•˜ì„¸ìš”\",\"í•˜ê¸°\",\"í•´\",\"ë©ë‹ˆë‹¤\",\"í•˜ì—¬\",'ì˜','ëœ','ë˜ê³ ','ë˜ì–´','ë˜ì—ˆìŠµë‹ˆë‹¤',\"ì—†ëŠ”\",\"ë“œë¦½ë‹ˆë‹¤\"\\\n",
    "                    ,\"ë˜ê¸°\",\"í•˜ì‹œëŠ”\",\"í•˜ê³ \",\"ì•Šì„\",\"ê°™ë‹¤\",\"ì‹¶ë‹¤\",\"ì´ëŸ°\",\"ì €ëŸ°\",\"ê·¸ëŸ°\",'ë°”ëë‹ˆë‹¤'\\\n",
    "                    ,\"í–ˆìŠµë‹ˆë‹¤\",\"í–ˆë‹¤\",\"í•´ë“œë¦½ë‹ˆë‹¤\",\"í•˜ì‹ \",\"í•˜ì‹¤\",\"ì•Šê³ \",\"í•´ìš”\",\"ê°€ëŠ¥í•©ë‹ˆë‹¤\",\"í•˜ê³ ì‹¶ìœ¼ì‹ \"\\\n",
    "                    ,\"ì•Šìœ¼ë©°\",\"ì£¼ì„¸ìš”\",\"ì˜¤ì„¸ìš”\"]\n",
    "\n",
    "stopwords_mecab = ['ìˆ˜','í€„ë¦¬í‹°','ë„ì‹œ','ë¶„','ì „ë¬¸','ìŠ¤íƒ€','ë…„','ì›',\\\n",
    "                   'ì›”','í™”','ìˆ˜','ëª©','ê¸ˆ','ì‹œ','ì•¤','ì¼','ê·¸ë¨','ë¬¸'] \n",
    "\n",
    "stopwords = stopwords_mecab + stopwords_Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-03 00:33:25,025 : INFO : saving Word2Vec object under my_second_model, separately None\n",
      "2020-02-03 00:33:25,026 : INFO : storing np array 'vectors' to my_second_model.wv.vectors.npy\n",
      "2020-02-03 00:33:25,323 : INFO : not storing attribute vectors_norm\n",
      "2020-02-03 00:33:25,324 : INFO : storing np array 'syn1neg' to my_second_model.trainables.syn1neg.npy\n",
      "2020-02-03 00:33:25,607 : INFO : not storing attribute cum_table\n",
      "2020-02-03 00:33:26,140 : INFO : saved my_second_model\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì´ë¦„ì„ ì§€ì •í•˜ê³  ì €ì¥í•œë‹¤.\n",
    "model_name = 'my_second_model'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥í•œ ëª¨ë¸ì€ ì—¬ê¸°ì„œ êº¼ë‚´ë©´ ëœë‹¤\n",
    "import pandas\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"my_second_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ì–´ ì‚¬ì „ ìˆ˜\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìš´ë™',\n",
       " 'ë‹¤ì´ì–´íŠ¸',\n",
       " 'ìš”ê°€',\n",
       " 'í•„ë¼í…ŒìŠ¤',\n",
       " 'ì›”',\n",
       " 'ì‹œ',\n",
       " 'ì¼',\n",
       " 'ìš´ë™í•˜ëŠ”ì—¬ì',\n",
       " 'ìˆ˜',\n",
       " 'ì¼ìƒ',\n",
       " 'a',\n",
       " 'ë”',\n",
       " 't',\n",
       " 'ë¶„',\n",
       " 'í•¨ê»˜']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¨ì–´ ì‚¬ì „ì—ì„œ ìƒìœ„ 10ê°œë§Œ ë³´ê¸°\n",
    "vocab = model.wv.vocab\n",
    "sorted(vocab, key=vocab.get, reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ìš´ë™': <gensim.models.keyedvectors.Vocab at 0x7f7ef474be50>,\n",
       " 'ë‹¤ì´ì–´íŠ¸': <gensim.models.keyedvectors.Vocab at 0x7f7ef474bf10>,\n",
       " 'ìš”ê°€': <gensim.models.keyedvectors.Vocab at 0x7f7ef48cf710>,\n",
       " 'í•„ë¼í…ŒìŠ¤': <gensim.models.keyedvectors.Vocab at 0x7f7e71733e10>,\n",
       " 'ì›”': <gensim.models.keyedvectors.Vocab at 0x7f7ef48fc550>,\n",
       " 'ì‹œ': <gensim.models.keyedvectors.Vocab at 0x7f7ef49d8b10>,\n",
       " 'ì¼': <gensim.models.keyedvectors.Vocab at 0x7f7ef49eb210>,\n",
       " 'ìš´ë™í•˜ëŠ”ì—¬ì': <gensim.models.keyedvectors.Vocab at 0x7f7ef474bed0>,\n",
       " 'ìˆ˜': <gensim.models.keyedvectors.Vocab at 0x7f7e71782150>,\n",
       " 'ì¼ìƒ': <gensim.models.keyedvectors.Vocab at 0x7f7e71733f10>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counterë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ë³´ê¸°\n",
    "from collections import Counter\n",
    "dict(Counter(vocab).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encourage'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°€ì¥ ì ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´\n",
    "min(vocab, key=vocab.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ì”¨í‹°í¬ë ˆìŠ¤í‹°ë²Œ', 0.9979838728904724),\n",
       " ('ê¸°ëŠ¥ì„±', 0.9974091649055481),\n",
       " ('íŒêµ', 0.9973074197769165),\n",
       " ('W', 0.9963376522064209),\n",
       " ('ê°€í¬ì™€', 0.9961326718330383),\n",
       " ('ê·¼ë§‰', 0.995990514755249),\n",
       " ('ê¹€í¬ìš”ê°€', 0.9957764148712158),\n",
       " ('ì í•‘í•', 0.9956825971603394),\n",
       " ('ì™¸ì—ë„', 0.9956777095794678),\n",
       " ('í™í•©ë°œë ˆ', 0.995653510093689)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('í™ë ˆ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ê·¼ìœ¡', 0.9995039701461792),\n",
       " ('ê°€ì¦ˆì•„', 0.9991205930709839),\n",
       " ('ìœ ì•„ì²´ìœ¡', 0.9989883899688721),\n",
       " ('í´ëŒ„ìŠ¤', 0.9987660646438599),\n",
       " ('ê´‘ì£¼', 0.9987014532089233),\n",
       " ('ë³µê·¼ìš´ë™', 0.9985569715499878),\n",
       " ('íƒœê¶Œë„', 0.9985353350639343),\n",
       " ('ëª¨ë¸', 0.9984759092330933),\n",
       " ('ìŠ¤íŒŒë§', 0.9983445405960083),\n",
       " ('MMA', 0.9982408881187439)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ë³µê·¼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ì—„ì²­', 0.998993992805481),\n",
       " ('ë“±ë“±', 0.9989675283432007),\n",
       " ('ë‹¤ë¦¬', 0.9986965656280518),\n",
       " ('í•˜ë„¤ìš”', 0.9986326694488525),\n",
       " ('í˜ë“¤ê²Œ', 0.9986114501953125),\n",
       " ('ê±±ì •', 0.9986083507537842),\n",
       " ('ë‹¤í–‰íˆ', 0.9985339641571045),\n",
       " ('ê³ ë§ˆì›Œ', 0.9985274076461792),\n",
       " ('í˜ë“¤ê³ ', 0.9983700513839722),\n",
       " ('í‰ìƒ', 0.9983628988265991)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('ì²™ì¶”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('íƒ€ë°”íƒ€ìš´ë™', 0.9963933229446411),\n",
       " ('í¼ìŠ¤ë„íŠ¸ë ˆì´ë„ˆ', 0.9963120222091675),\n",
       " ('ê·¼ë ¥ìš´ë™', 0.9958586692810059),\n",
       " ('ì›¨ì´íŠ¸', 0.9956011772155762),\n",
       " ('ì•ˆë‹¤ë¥´', 0.9955255389213562),\n",
       " ('í™ˆíŠ¸', 0.995509922504425),\n",
       " ('ë°”ë¥¸ìì„¸', 0.9954653382301331),\n",
       " ('ìê¸°ê´€ë¦¬', 0.9952776432037354),\n",
       " ('ê·¼ìœ¡ê¸°ëŠ¥ìš´ë™', 0.9952510595321655),\n",
       " ('ì²´ë ¥ì¦ì§„', 0.9951571822166443)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['ì²™ì¶”','ê±´ê°•'], negative=['ìš°í•œ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ìš´ë™ì˜ìƒ', 0.9981263875961304),\n",
       " ('ëŒ€í•œí´ëŒ„ìŠ¤ì—°ë§¹', 0.9980324506759644),\n",
       " ('íŒíŒ', 0.9976912140846252),\n",
       " ('ì¶•êµ¬', 0.9974087476730347),\n",
       " ('ì•„ì¹¨ìš´ë™', 0.9973322153091431),\n",
       " ('ì²´ë ¥', 0.9972175359725952),\n",
       " ('ìš´ë™ì‹œì‘', 0.9970873594284058),\n",
       " ('pointgym', 0.9970570802688599),\n",
       " ('ì í•‘', 0.9969454407691956),\n",
       " ('ì‘ì‹¬ì‚¼ì¼', 0.9968618750572205)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['ì²™ì¶”','ê±´ê°•'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884146"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('ì²™ì¶”', 'ê²€ë„')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PT',\n",
       " 'ê²€ë„',\n",
       " 'ê¸°êµ¬í•„ë¼í…ŒìŠ¤',\n",
       " 'ë‹¤ë¹ˆì¹˜ë°”ë””ë³´ë“œ',\n",
       " 'ë“œëŸ¼ìŠ¤í‹±',\n",
       " 'ë“±ì‚°',\n",
       " 'ë¼í‹´ëŒ„ìŠ¤',\n",
       " 'ë§¤íŠ¸í•„ë¼í…ŒìŠ¤',\n",
       " 'ë®¤ì§ë³µì‹±',\n",
       " 'ë°”ì°¨íƒ€']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_to_loop = df[\"exercise_name\"].to_list()\n",
    "\n",
    "\n",
    "exercise_to_loop.sort()\n",
    "exercise_to_loop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_val = []\n",
    "\n",
    "def tuple(exercise_name,word):\n",
    "    return (word, model.wv.similarity(exercise_name,word))\n",
    "\n",
    "def show_similarity_for_each_exercise(word):\n",
    "    for item in exercise_to_loop:\n",
    "        vector_val.append(tuple(word, item))\n",
    "    return vector_val\n",
    "\n",
    "vector_list = show_similarity_for_each_exercise('ë³µê·¼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('í´ëŒ„ìŠ¤', 0.9987661),\n",
       " ('í”Œë¼ì‰í•„ë¼í…ŒìŠ¤', 0.9956836),\n",
       " ('í¬ë¡œìŠ¤í•', 0.99524415),\n",
       " ('ë°œë ˆ', 0.99482024),\n",
       " ('ì‚¬ì´í´', 0.9946428),\n",
       " ('ìš”ê°€ì¿ ì•„', 0.9944515),\n",
       " ('ì¡°ê¹…', 0.9943916),\n",
       " ('ë³µì‹±', 0.99346507),\n",
       " ('ìˆ˜ì˜', 0.99276334),\n",
       " ('íŒ¨ë“¤ë³´ë“œ', 0.9924408)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Sort(tup): \n",
    "    return(sorted(tup, key = lambda x: float(x[1]), reverse = True)) \n",
    "\n",
    "Sort(vector_list)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
